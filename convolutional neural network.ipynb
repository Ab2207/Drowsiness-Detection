{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "np.random.seed(123)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import itertools\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical  # use for converting labels to one-hot-encoding\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers.normalization  import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowsy0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>drowsy1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>drowsy2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>drowsy3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>drowsy4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id image_id  label\n",
       "0   0  drowsy0      0\n",
       "1   1  drowsy1      0\n",
       "2   2  drowsy2      0\n",
       "3   3  drowsy3      0\n",
       "4   4  drowsy4      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df = pd.read_csv(\"/Users/harit/Desktop/images_excel.csv\")\n",
    "images_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109669\n"
     ]
    }
   ],
   "source": [
    "path  = []\n",
    "\n",
    "\n",
    "for i in images_df['id']:\n",
    "    if i < 57745:\n",
    "        path.append(\"/Users/harit/test_data_final/drowsy\"+str(i)+\".jpg\")\n",
    "    else:\n",
    "        path.append(\"/Users/harit/test_data_final/vigilant\"+str(i)+\".jpg\")\n",
    "    while i > 109668:\n",
    "        break\n",
    "        \n",
    "print(len(path))\n",
    "images_df['path'] = path\n",
    "images_df = shuffle(images_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dict = {\n",
    "    '0': 'Drowsy',\n",
    "    '1': 'Vigilant'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def Dataset_loader():\n",
    "    IMG = []\n",
    "    read = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n",
    "    for IMAGE_NAME in images_df['path']:\n",
    "        img = read(IMAGE_NAME)\n",
    "        img = cv2.resize(img, (100,75))\n",
    "        IMG.append(img)\n",
    "    return IMG\n",
    "images_df['image'] = Dataset_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92672    [[[176, 146, 141], [189, 175, 172], [190, 175,...\n",
      "53961    [[[209, 205, 227], [206, 207, 227], [206, 210,...\n",
      "69246    [[[190, 180, 179], [191, 182, 175], [192, 184,...\n",
      "93130    [[[175, 147, 141], [189, 175, 172], [187, 178,...\n",
      "72747    [[[191, 181, 189], [194, 184, 192], [195, 184,...\n",
      "                               ...                        \n",
      "63206    [[[201, 190, 195], [203, 190, 199], [201, 190,...\n",
      "61404    [[[210, 199, 203], [210, 199, 203], [211, 200,...\n",
      "17730    [[[200, 196, 197], [203, 199, 200], [203, 198,...\n",
      "28030    [[[179, 187, 200], [184, 194, 204], [184, 194,...\n",
      "15725    [[[151, 129, 124], [149, 132, 125], [170, 159,...\n",
      "Name: image, Length: 109669, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(images_df['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = images_df['image']\n",
    "y = images_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_o, x_test_o, y_train_o, y_test_o = train_test_split(x, y, test_size=0.10,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train_o.tolist())\n",
    "x_test = np.asarray(x_test_o.tolist())\n",
    "\n",
    "#x_train_mean = np.mean(x_train)\n",
    "#x_train_std = np.std(x_train)\n",
    "\n",
    "#x_test_mean = np.mean(x_test)\n",
    "#x_test_std = np.std(x_test)\n",
    "\n",
    "#x_train = (x_train - x_train_mean)/x_train_std\n",
    "#x_test = (x_test - x_test_mean)/x_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train_o, num_classes = 2)\n",
    "y_test = to_categorical(y_test_o, num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.1, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 75, 100, 32)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 100, 32)       9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 37, 50, 12)        3468      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 37, 50, 12)        1308      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 25, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                64812     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 79,758\n",
      "Trainable params: 79,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras import backend as K\n",
    "import itertools\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "input_shape = (75,100,3)\n",
    "num_classes = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=input_shape))\n",
    "model.add(Conv2D(32,kernel_size=(3, 3), activation='relu',padding = 'Same',))\n",
    "model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(12, (3, 3), activation='relu',padding = 'Same'))\n",
    "model.add(Conv2D(12, (3, 3), activation='relu',padding = 'Same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001)\n",
    "\n",
    "#, beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = opt , loss = \"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 88831 samples, validate on 9871 samples\n",
      "Epoch 1/5\n",
      "88831/88831 [==============================] - 1345s 15ms/step - loss: 7.3071 - accuracy: 0.5130 - val_loss: 7.1861 - val_accuracy: 0.5315\n",
      "Epoch 2/5\n",
      "88831/88831 [==============================] - 1160s 13ms/step - loss: 7.2090 - accuracy: 0.5266 - val_loss: 7.1861 - val_accuracy: 0.5315\n",
      "Epoch 3/5\n",
      "88831/88831 [==============================] - 1147s 13ms/step - loss: 7.2166 - accuracy: 0.5258 - val_loss: 7.1861 - val_accuracy: 0.5315\n",
      "Epoch 4/5\n",
      "88831/88831 [==============================] - 1112s 13ms/step - loss: 7.2211 - accuracy: 0.5255 - val_loss: 7.1861 - val_accuracy: 0.5315\n",
      "Epoch 5/5\n",
      "88831/88831 [==============================] - 1102s 12ms/step - loss: 7.2155 - accuracy: 0.5265 - val_loss: 7.1861 - val_accuracy: 0.5315\n"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "batch_size = 2000\n",
    "\n",
    "history = model.fit(x_train,y_train, batch_size=batch_size,\n",
    "                              epochs = epoch, validation_data = [x_validate,y_validate],\n",
    "                              verbose = 1,\n",
    "                              )\n",
    "\n",
    "#callbacks=[learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10967/10967 [==============================] - 19s 2ms/step\n",
      "9871/9871 [==============================] - 16s 2ms/step\n",
      "Validation: accuracy = 0.531456  ;  loss_v = 7.186107\n",
      "Test: accuracy = 0.521200  ;  loss = 7.343401\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "loss_v, accuracy_v = model.evaluate(x_validate, y_validate, verbose=1)\n",
    "print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "model.save(\"detection_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(x_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the confusion matrix\n",
    "cm_plot_labels = ['Drowsy', 'Vigilant']\n",
    "\n",
    "plot_confusion_matrix(confusion_mtx, cm_plot_labels, title='Confusion Matrix')\n",
    "#plot_confusion_matrix(confusion_mtx, classes = range(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/AB/Desktop/data/vigilant48412.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3de8ebb3a871>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimg_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Users/AB/Desktop/data/vigilant48412.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mimg_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimg_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'L'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2877\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2878\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2879\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/AB/Desktop/data/vigilant48412.jpg'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "img_pred = image.load_img(\"/Users/AB/Desktop/data/vigilant48412.jpg\", target_size = (75, 100))\n",
    "img_pred = image.img_to_array(img_pred)\n",
    "img_pred = np.expand_dims(img_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]]\n",
      "vigilant\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(img_pred)\n",
    "print(result)\n",
    "\n",
    "if result[0][0] == 0:\n",
    "    prediction = 'drowsy'\n",
    "else:\n",
    "    prediction = 'vigilant'\n",
    "    \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
